{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-ZuyOUX69WJ",
        "outputId": "4f9dbbce-53eb-4ace-e0c0-587cce07f91c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UdNGgZz7CHA",
        "outputId": "a6928a2f-d68b-4f02-a143-14a6e29fa767"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "BcMqIdnW7CDh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Question Answering\n",
        "\n",
        "QA_input = [{'question': 'Why is conversion important?',\n",
        "             'context': 'The option to convert models between FARM and transformers gives freedom to the user between frameworks. This flexibility allows developers to leverage the strengths of both FARM and transformers, ensuring they can choose the most suitable framework for their specific use case, whether its for fine-tuning, deploying, or scaling their models.'},\n",
        "            {'question': 'How many programming languages does BLOOM support?',\n",
        "             'context': 'BLOOM has 176 billion parameters and can generate text in 46 natural languages, 13 programming languages. Additionally, BLOOM is capable of understanding and generating code in several programming languages, making it a versatile tool for both natural language processing and code-related tasks.'}]\n",
        "\n"
      ],
      "metadata": {
        "id": "2g5DE_dd7CBE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'google-bert/bert-large-uncased-whole-word-masking-finetuned-squad'"
      ],
      "metadata": {
        "id": "IC36Npm57B-w"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GJsTt9G_Jwe",
        "outputId": "5fe54c7b-7cfb-42d9-9ac5-7fd1fe3800d6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google-bert/bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs0 = tokenizer(QA_input[0]['question'], QA_input[0]['context'], return_tensors='pt')\n",
        "output0 = model(**inputs0)"
      ],
      "metadata": {
        "id": "ysWMNplJ_Js9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs1 = tokenizer(QA_input[1]['question'], QA_input[1]['context'], return_tensors='pt')\n",
        "output1 = model(**inputs1)"
      ],
      "metadata": {
        "id": "6gUFfsNo_Jqg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bG1_L-w_JnM",
        "outputId": "0b018c49-118b-43f2-b095-48b410e30072"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-5.1694, -7.2588, -8.1341, -8.1158, -8.3211, -9.6844, -5.1694,  2.5985,\n",
              "         -0.0413, -2.8557, -0.8444, -2.6945, -4.2397, -3.8757, -6.7293, -2.7208,\n",
              "          4.0920,  3.0154, -3.5387, -1.6441, -1.6790, -1.9555, -1.6900, -4.3561,\n",
              "         -5.1699,  3.5866,  4.4503,  4.1822,  3.1130,  3.0333,  2.9694,  0.2014,\n",
              "          0.7054, -4.4236, -2.6305, -2.0688, -5.1079, -1.9380, -2.0160,  4.1415,\n",
              "          2.2241,  0.6583,  1.5961, -0.5297, -0.7798, -1.4027, -1.2598, -2.7575,\n",
              "         -1.6978, -1.2747, -2.4639, -3.3126, -4.7845, -0.0767, -3.7406,  0.2447,\n",
              "          0.0943, -5.5910, -3.7319, -7.5494, -2.4475, -5.4095, -8.1249, -7.2322,\n",
              "         -1.9117, -5.6670, -2.8614, -5.8391, -5.1688]],\n",
              "       grad_fn=<CloneBackward0>), end_logits=tensor([[ 0.2403, -7.2084, -7.4871, -8.1762, -7.3157, -7.1659,  0.2404, -5.3968,\n",
              "         -3.2094, -7.0399, -4.9628, -4.6982, -6.9677, -5.1944, -7.0367, -2.7582,\n",
              "         -4.0829,  1.4784, -4.2288, -4.7478,  2.6039, -2.6883, -1.1300,  4.8410,\n",
              "          0.2403, -4.6427,  2.1958, -2.8486, -0.3949, -2.6294, -0.5461, -3.8830,\n",
              "          2.1500, -3.4648, -3.0235, -0.1572, -4.0849,  4.6875, -0.0927, -1.7223,\n",
              "         -3.4548, -2.7782, -1.1610, -3.9562, -2.9603, -1.0304,  2.8764, -2.9940,\n",
              "         -3.9542, -2.3921, -1.4011,  4.8808,  2.3971, -5.0379, -4.6209, -4.6144,\n",
              "         -4.6010, -5.9302,  0.5220, -4.5560, -4.5695, -2.3750, -3.2505, -6.3503,\n",
              "         -1.6501, -4.8295,  4.6565,  3.0700,  0.2400]],\n",
              "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgI6A3TnAQXh",
        "outputId": "cee1ce12-f32c-427c-b20f-e2573102f36a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-6.5057, -6.8529, -7.8460, -8.0712, -8.4338, -7.5633, -7.6696, -7.8239,\n",
              "         -8.5769, -6.5056, -2.4089, -6.3738, -2.0970, -6.0311, -5.8122, -7.3099,\n",
              "         -4.4952, -5.3033, -5.4910, -7.3096, -1.5077, -6.4979, -6.6752, -5.2407,\n",
              "          5.4560, -4.5626, -5.1111, -6.5056, -6.1010, -7.4115, -3.1303, -7.3191,\n",
              "         -5.1122, -7.8144, -5.5715, -7.9048, -5.1907, -5.1064, -7.0877, -1.3883,\n",
              "         -4.3113, -5.6011, -7.4397, -6.5982, -7.1660, -7.7989, -6.2851, -7.4093,\n",
              "         -8.1901, -7.8000, -6.2740, -7.1585, -7.3050, -8.7493, -6.1147, -8.2026,\n",
              "         -8.4260, -7.2735, -7.1656, -6.5058]], grad_fn=<CloneBackward0>), end_logits=tensor([[-2.3166, -6.5612, -6.3200, -6.4718, -6.4147, -6.6876, -6.7462, -6.0614,\n",
              "         -6.3724, -2.3165, -2.7717, -6.0409, -2.4968, -1.9681, -2.2747, -4.7559,\n",
              "         -6.1115, -5.3174, -4.3518, -6.1997, -0.8406, -4.2812, -2.3979, -3.5253,\n",
              "          6.3917, -1.2915,  2.0744, -2.3165, -5.1747, -4.7515, -3.2817, -6.4705,\n",
              "         -6.1733, -6.3700, -5.7448, -7.0278, -6.2940, -4.1336, -6.1783, -0.6620,\n",
              "         -3.9734, -1.6273, -2.1241, -6.5224, -5.2357, -6.9116, -5.8119, -5.2560,\n",
              "         -7.0282, -7.2057, -5.7069, -4.2150, -4.6084, -7.2412, -5.2405, -6.1521,\n",
              "         -6.3165, -4.1871, -3.0407, -2.3169]], grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_start_idx1 = torch.argmax(output0.start_logits)\n",
        "answer_end_idx1 = torch.argmax(output0.end_logits)\n",
        "\n",
        "answer_tokens1 = inputs0.input_ids[0, answer_start_idx1 : answer_start_idx1 + 1]\n",
        "answer1 = tokenizer.decode(answer_tokens1)\n",
        "\n",
        "print(\"ques: {}\\n answer: {}\".format(QA_input[0]['question'], answer1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQB8r1C4ARmr",
        "outputId": "f562b763-04bc-45ce-a0ce-dec96987aaa3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ques: Why is conversion important?\n",
            " answer: flexibility\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(QA_input[0]['question']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLkbxBXSAWyX",
        "outputId": "6b21e049-2c12-4ec2-efd3-53d4bf5e57f2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_start_idx2 = torch.argmax(output1.start_logits)\n",
        "answer_end_idx2 = torch.argmax(output1.end_logits)\n",
        "\n",
        "answer_tokens2 = inputs1.input_ids[0, answer_start_idx2 : answer_start_idx2 + 1]\n",
        "answer2 = tokenizer.decode(answer_tokens2)\n",
        "\n"
      ],
      "metadata": {
        "id": "VMC2X31cBqkT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ques: {}\\n answer: {}\".format(QA_input[1]['question'] , answer2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_qTeBFSAWu8",
        "outputId": "43a8b740-3b83-487b-90cc-9442929aa55c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ques: How many programming languages does BLOOM support?\n",
            " answer: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HcbG1OEOAWsz"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}